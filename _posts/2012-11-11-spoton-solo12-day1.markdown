---
layout: post
title: SpotOn day 1. 
categories: 
- #solo12
- STM
- conference
- solo
- spoton
- London
- publishing
---

I'll keep a partial, live-idsh blog going during the day. I've been going to these things, I think, since about 2008. I really like these meetings. I'm going to probably keep these notes pretty lightweight. 

## sessions<a id="top">:</a>

- [Ben Goldacre](#ben)
- [Kamila ](#front)


## [Ben Goldacre][bg] - opening keynote, on data. <a id="ben">.</a> ([top](#top))

Ben is talking about the issues that arise when you place a lot of data in one place. Modern tools that allow large data sets to be indexable create a new thing, a thing that not many people understand. He makes the analogy between bringing many atoms of uranium together, this creates a qualatativley different thing. Dangerous, but powerful. Data has a similar property. (We often manage to ignore the potential power that the elves at google have). He mentions the new data store on health data.

For individual studies, we are learning ways to slightly distort the data to make it more useful. Bringing a lot of data together allows you to start looking for patterns. Sometimes you are only seeing faces in the cloud, but sometimes you do see real patterns. HRT is an interesting example where just observing leads you down very much the wrong path. You need to have randomised control trials. 

There is a great example of giving steroids to people with head injuries coming into A&E. There were no randomised trials, and people were unwilling to not give steroids, as there was a belief that it worked. When you are facing a person who is dying on the table it is very hard to take a step back and make the decision to not give the intervention _even if there was no evidence that the intervention worked_. When they finally did the trial they discovered that people who were given the steroids were more likely to die. 

(Although not as dramatic, by a long way this issue of trying randomised trials is something that is also hard in a business context, as large companies start to increase the cost of trying small iterations, one of the issues being the overhead that comes along with increased amounts of communication with a larger number of stakeholders). 

Ben mentions the problem also extends to government policy. Government tends to like to do pilot studies, and not controlled trialls. He was involved in the writing of a [white paper][pol] on the value of randomised trials in determining the effectiveness of policy. (an old [climbing partnerr][dh] of mine works on these issues, it's interesting to look at existing data to see where policy has a real effect). 

One of the threads of the talk is looking at data a low cost (how do you reduce the cost of iterating, and finding out whether what you are doing is working? - a common call in the startup scene, however it should be said that we often plough on regardless of the evidence, and just like doing what we like doing, it's really hard to pay attention to the data). 

How do you design trials that are cheap, that show you how to find those small improvements? 

At the moment we are already collecting a lot of data in health system. If you take the opportunity to put a little bit of structure at the beginning you can get a lot of valuable information downstream. They are doing a trial by looking at [statins][st]. 

The idea is to randomly assign a statin to people at the doctor, and track this at the point of assignment. (there is no current information on which statin is better, and they are effectively being assigned randomly, but with no tracking). By adding this small piece of structure at the beginning you end up with a nice randomised trial. 

If you could impose this structure at any point where there was uncertainty in efficacy of interventions, you could turn the whole of the NHS into a testing machine for finding those 1% improvements, that collectivity can have en enormous positive effect on our health.

Ben talks about the need to have tools that can find good content to read, over the crud, in the same way that we want to find good studies, over shit studies. (They are about to launch a project with the open data foundation that looks at waste that is happening at the point of prescription). 

He discusses [altmetrics][alm] - briefly. 

(Of course these issues of looking for good filters, and finding context at point of consumption, is an interesting topic, that is top of mind for a lot of us).

[bg]: http://www.badscience.net/
[pol]: http://www.cabinetoffice.gov.uk/resource-library/test-learn-adapt-developing-public-policy-randomised-controlled-trials
[dh]: http://sm.psc.isr.umich.edu/debra/
[st]: http://en.wikipedia.org/wiki/Statin 
[alm]: http://altmetrics.org/manifesto/



## [Kamila Markram][km] - publishing science in the internet age <a id="front">.</a> ([top](#top))

Kamila is one of the founders of [frontiers][front]. She starts by talking about some trends with science online. A trend is facebook, most scientists are now there, however both facebook and linkedin do not cater the direct needs of scientists. (I have some opinions about the value of online scientific networks and the difficulty of building these things). 

The [frontiers network][fn] immediately increased pageviews on articles and author profiles (+70% on some metrics). (this is pretty impressive). 

She mentions that google docs is being used to collaboratively creating papers. IMO getting the authoring tools fixed is one of the big areas of schlep/opportunity in the science communication space. 

(There are a very large number of services being mentioned in this talk, I have heard of most of them, but there seems to be too many for me to grok, and check on whether or not I know them). 

It also looks like frontiers is built on top of ASP, that is one stack that I have very little experience of. 

Kamila starts talking about publishing trends. There are some comments about open access. She is touching on the issue of bias in peer review. For sure, peer review is this weird process, and there is a lot of bias. ("~~Fixing~~"/Augmenting peer review, let's do that - actually, the issue is the social aspect, it's soylent green, it's totally made up by researchers themselves, it was put to me that the least important community is are the senior academics, as they are the ones that will die soonest).

 


[km]: http://bluebrain.epfl.ch/page-68308-en.html
[front]: http://www.frontiersin.org/
[fn]: http://www.frontiersin.org/events/all_events
[sg]: http://en.wikipedia.org/wiki/Soylent_Green

