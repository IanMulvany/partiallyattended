<head>
	
	<meta name="generator" content="Hugo 0.73.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta property="og:title" content="mini-review of &#34;Measuring the User Experience on a Large Scale&#34;" />
<meta property="og:description" content="Measuring the User Experience on a Large Scale: User-Centered Metrics for Web Applications by Kerry Rodden, Hilary Hutchinson, and Xin Fu
Rodden, Hutchinson and Fu describe a framework for measuring the user experience of web apps through mining server logs. Since they are based at Google one assumes that the framework that they are describing has been battle tested.
They focus on metrics that measure user centric aspects of the online experience, in contrast to business centric (such as PULSE metrics: Page-views, Uptime, Latency, Seven day actives, Earnings)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://partiallyattended.com/2011/03/29/measuring_ux/" />
<meta property="article:published_time" content="2011-03-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2011-03-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="mini-review of &#34;Measuring the User Experience on a Large Scale&#34;"/>
<meta name="twitter:description" content="Measuring the User Experience on a Large Scale: User-Centered Metrics for Web Applications by Kerry Rodden, Hilary Hutchinson, and Xin Fu
Rodden, Hutchinson and Fu describe a framework for measuring the user experience of web apps through mining server logs. Since they are based at Google one assumes that the framework that they are describing has been battle tested.
They focus on metrics that measure user centric aspects of the online experience, in contrast to business centric (such as PULSE metrics: Page-views, Uptime, Latency, Seven day actives, Earnings)."/>

	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/all.css" />
	<title>mini-review of &#34;Measuring the User Experience on a Large Scale&#34; | Partially Attended</title></head>


<article id="content">

<h1 class="post-title">mini-review of &#34;Measuring the User Experience on a Large Scale&#34;</h1>

<section>
      <h4 id="date"> Tue Mar 29, 2011 </h4>
      <h5 id="wordcount"> 332 Words </h5>
</section>

<div class="post-entry-tags">
                          Tags:
                           <a href="/categories/paper">paper</a>, <a href="/categories/user-experience">user experience</a>, <a href="/categories/data">data</a>, <a href="/categories/google">google</a>
</div>

<div>
          <p><a href="http://www.mendeley.com/research/measuring-user-experience-large-scale-usercentered-metrics-web-applications/">Measuring the User Experience on a Large Scale:
User-Centered Metrics for Web Applications</a> by Kerry Rodden, Hilary Hutchinson, and Xin Fu</p>
<p>Rodden, Hutchinson and Fu describe <a href="http://www.mendeley.com/research/measuring-user-experience-large-scale-usercentered-metrics-web-applications/">a framework for measuring the user experience of web apps through mining server logs</a>. Since they are based at Google one assumes that the framework that they are describing has been battle tested.</p>
<p>They focus on metrics that measure user centric aspects of the online experience, in contrast to business centric (such as PULSE metrics: Page-views, Uptime, Latency, Seven day actives, Earnings). They maintain that the metrics they propose are better suited to products that have been launced, rather than being used in the design stage. They maintain that these metrics should be used in conjunction with other measures, such as direct user studies.</p>
<p>They name their metrics HEART metrics: Happiness, Engagement, Adoption, Retention, and Task success.</p>
<p>Happiness is measured through user surveys. An interesting insight is that the iGoogle team introduced a new design, and saw a drop in happiness scores with the design, but these improved over time, indicating that the initial drop was due to change aversion. They measure this score weekly through in-app surveys that can be deployed at scale, and so they have a historic record of user happiness.</p>
<p>Engagement is some activity per volume of users, such as average number of visits or actions per user over a time frame. This is best reported on an average per user basis.</p>
<p>Adoption and retention are rather self-evident metrics.</p>
<p>Task success seems more problematic to track, as one needs to set up the metric around task success beforehand, and then A/B test a set of potential variable scenarios.</p>
<p>Overall this is a nice little paper that articulates well the challenges around understanding user interaction with one&rsquo;s site. The clear challenge remains deciding what one&rsquo;s goals are, and then coming to an agreement about what the signals are that can be tracked that will inform you about whether your goals are being met.</p>

 </div>

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/">This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>

 </article>








