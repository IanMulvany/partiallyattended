<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.73.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="keywords" content="hugo,vitae,theme,static"><meta name="description" content=""><meta property="og:title" content="choice magazine podcast Questions" />
<meta property="og:description" content="This week I was interviewed for the ALA choice podcast, a podcast that the that is
 a weekly program featuring in-depth conversations about contemporary trends, best practices, and case studies important to academic librarians. Hosted by Bill Mickey, the Editorial Director at Choice
 The topic was about trends in big data and the role of the library, and it was really fun to participate in, and the panel I was on included Caroline Muglia from the University of Southern California Libraries as well as Andy Rutkowski and Eimmy Karina Solis from USC libraries." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://partiallyattended.com/2018/05/04/choice_magazine_podcast_questions_/" />
<meta property="article:published_time" content="2018-05-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-05-04T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="choice magazine podcast Questions"/>
<meta name="twitter:description" content="This week I was interviewed for the ALA choice podcast, a podcast that the that is
 a weekly program featuring in-depth conversations about contemporary trends, best practices, and case studies important to academic librarians. Hosted by Bill Mickey, the Editorial Director at Choice
 The topic was about trends in big data and the role of the library, and it was really fun to participate in, and the panel I was on included Caroline Muglia from the University of Southern California Libraries as well as Andy Rutkowski and Eimmy Karina Solis from USC libraries."/>

	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/all.css" />
	<link rel="stylesheet" href="/css/katex.min.css" crossorigin="anonymous">
	<script defer src="/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>choice magazine podcast Questions | Partially Attended</title></head>
<body><header>
	
	<div id="titletext"><h2 id="title"><a href="http://partiallyattended.com">Partially Attended</a></h2></div>
	<div id="title-description"><p id="subtitle">an irregularly updated blog by Ian Mulvany</p><div id="social">
			<nav>
				<ul>
					<li><a href="https://github.com/dataCobra/hugo-vitae"><i title="Github" class="icons fab fa-github"></i></a></li>
					<li><a href="/index.xml"><i title="RSS" class="icons fas fa-rss"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/about">About</a></li>
				
				<li><a href="/categories">Posts by Tag</a></li>
				
				<li><a href="/tags">Tags</a></li>
				
				<li><a href="/post">All Posts</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main>

<article id="content">

<h1 class="post-title">choice magazine podcast Questions</h1>

<section>
      <h4 id="date"> Fri May 4, 2018 </h4>
      <h5 id="wordcount"> 2026 Words </h5>
</section>

<div class="post-entry-tags">
                          Tags:
                           <a href="/categories/computational-socical-science">computational-socical-science</a>, <a href="/categories/research">research</a>, <a href="/categories/big-data">big-data</a>, <a href="/categories/tools">tools</a>, <a href="/categories/software">software</a>
</div>

<div>
          <p>This week I was interviewed for the <a href="http://www.ala.org">ALA</a> choice podcast, a podcast that the that is</p>
<blockquote>
<p>a weekly program featuring in-depth conversations about contemporary trends, best practices, and case studies important to academic librarians. Hosted by Bill Mickey, the Editorial Director at Choice</p>
</blockquote>
<p>The topic was about trends in big data and the role of the library, and it was really fun to participate in, and the panel I was on included Caroline Muglia from the University of Southern California Libraries as well as Andy Rutkowski and Eimmy Karina Solis from USC libraries.</p>
<p>We got a slate of questions to look through before the podcast. We didn’t get through all of these during the podcast, but I sketched out replies to some of them. Also, during the podcast the actual answers that I gave to some of these questions ended up being a bit different, but in any case, here are the thoughts I had ahead of the podcast session to the topics we covered:</p>
<p><em>Episode 1: Defining and Contextualizing Big Data</em></p>
<ol>
<li><em>The term big data can mean a variety of things. What are some of the ways we can define big data? Are there fundamental elements that these definitions have in common? And what does big data mean in an academic context? Ian, how are you defining it at SAGE? Caroline, likewise, how has USC defined it in a way that works for you?</em></li>
</ol>
<p>Matt Salganik (Princeton) has a great overview of attributes of big data sets in the social sciences — (<a href="http://www.bitbybitbook.com/en/1st-ed/observing-behavior/characteristics/">Bit By Bit - Observing behavior - 2.3 Ten common characteristics of big data</a>) — where he talks about the difference between repurposing found data, and collecting data specifically for your own needs. He also provides a great analysis of attributes of data that are helpful for social research, like being always on, non-reactive, and being big enough to be able to observe rare events.</p>
<p>In contrast there are lots of kinds of attributes that are less helpful, like being incomplete, inaccessible, changing over time.</p>
<p>But even before getting into these kinds of definitions, we think at SAGE that there is a basic level of data management and competence, that applies equally to big and small data, that would be helpful for researchers to achieve. I like to think about it in terms of asking at what point does the size of the data present a problem for the researcher. That specific point varies depending on the technical competence of the researchers. We should be aiming to raise that level of competence across the board.</p>
<ol start="2">
<li><em>Ian, why is it we’re talking about big data now? What’s been happening in the broader tech market that’s brought this concept “mainstream?”</em></li>
</ol>
<p>There have been three trends here, the cost of storage has plummeted, the cost of computing power has also dropped, making it cheap to store huge amounts of data, and cheap to process that data, in near real time.</p>
<p>Combined with that the wider industry has created a ton of open source tools that make is possible to work with data at this scale, from things like HBase, to MongoDB and Apache Spark, through to highly preformant libraries for doing machine learning on larger data sets, like sk-learn in python.</p>
<p>Initially all of these tools required quite a bit of setup to get working, but increasingly large platform vendors like Google Microsoft and Amazon are making these tools available in a platform as a service model, where you just have to bring your data and your research question, and they provide the storage, compute and machine learning models to run over that data.</p>
<p>This move to commoditisation of technology is nicely captured by Wardley Value Maps - <a href="https://www.cio.co.uk/it-strategy/introduction-wardley-value-chain-mapping-3604565/">An introduction to Wardley ‘Value Chain’ Mapping | IT Strategy | CIO UK</a></p>
<ol start="3">
<li><em>Many of the techniques for mining and utilizing big data come from the STEM fields. Could you talk a little about what a traditional definition of big data is in the STEM environment? How has this helped to push big data into the humanities and social sciences?</em></li>
</ol>
<p>Up to now I think its more the tools and techniques for working with data that have started to move across. The actual data sets are obviously quite different. Critically there are a host of ethical and consent issues that arise when the core units of study are people and not just atoms.</p>
<p>At SAGE we think that social sciences may go through an evolution like we saw in the life Sciecne with the advent of bioinformatics and the need for specialised skills to work with this kind of data.</p>
<p>We recently launched <a href="https://ocean.sagepub.com/concept-grants/">Concept Grants</a> to support researchers who are creating tooling to help with this transition, and one of the first winners are  Stefano Cresci and Maurizio Tesconi from the Institute for Informatics and Telematics, Italian National Research Council for building a tool — Digital DNA Toolbox — that helps to make bioinformatics algorithms work on social data sets.</p>
<ol start="4">
<li><em>What specific skills, tools, and best practices are transforming big data research in the humanities and social sciences?</em></li>
</ol>
<p>Many of the skills you need to work with these kinds of data are skills that might traditionally have come from computer science. That’s why at SAGE we have launched a series of online courses - <a href="https://campus.sagepub.com">SAGE Campus</a> - specifically designed to teach social scientists how to program.</p>
<p>That said we don’t think that these new skills are going to replace the need for mastering the existing skills in the field.</p>
<p>It’s probably best to think of social data at scale as acting like the development of a new kind of tool. My background is in astronomy, and there the analogy would be that any time a telescope was built that could detect a new wavelength of light, our understanding of the universe increased. It didn’t replace any of the exiting technique or knowledge, but just augmented those. Likewise in the social sciences the availability of massive social data sets can augment the tools and techniques and knowledge that the field has already developed.</p>
<p>The twitter historical record, for example, can let you see how populations interacted before, during and after specific events. It can almost acts like a social time machine.</p>
<ol start="5">
<li><em>What are some of the transformations those fields are seeing because of these new techniques?</em></li>
</ol>
<p>I think there is an increased interest in collaborations around data at scale across different boundaries, from government to private companies to the academy.</p>
<p>Some people have voiced a concern that maybe the big tech giants are going to poach all of the technically competent social scientists, but we are seeing a lot of universities rolling out masters level courses in computational social Sciecne, so there is a clear effort to meet that demand.</p>
<p>I think projects like Julia Lanes’ <a href="https://coleridgeinitiative.org">The Coleridge Initiative</a> which brings together academics and people from government to develop data analysis skills is really interesting.</p>
<p>Also the recently announced initiative to make Facebook data available to researchers with the SSRC acting as a mediator is clearly an important and interesting development.</p>
<p>Episode 2: Big Data and the Library</p>
<ol start="4">
<li><em>How does, or should, big data or collections data come up in vendor meetings? Some get it, some don’t. Sometimes what might seem like a simple request from the library is very difficult on the vendor side to accommodate. At the collection level, what does USC (or other libraries) need from its vendors in order to readily facilitate big data research?</em></li>
</ol>
<p>SAGE allows data mining of our content. My colleagues tell me that</p>
<blockquote>
<p>If a user has access to an article on SJ, they can download it via the CrossRef API and perform text mining on it for academic/educational purposes. If users want to perform commercial text mining, we also participate in the CCC’s Rightfind XML service, which is a search and discovery platform that allows you to mass download life science articles for mining. We think it’s better to participate in cross-publisher services than offer our own API, which a researcher would have to learn separately.</p>
</blockquote>
<blockquote>
<p>For non-journal content like SK and SRM, we don’t offer an API but we do still allow users to download and mine content for non-commercial use. They either have to screen scrape or ask us for help.</p>
</blockquote>
<ol start="8">
<li><em>Ian, what is Sage looking to develop to fit into these sorts of user workflows? What sorts of strategic questions is Sage pursuing? What areas within the academic setting are most interesting to you right now?</em></li>
</ol>
<p>At the moment there are a couple of answers to that.</p>
<p>In the researcher setting with <a href="https://ocean.sagepub.com">SAGE Ocean</a> we are focussing on figuring out how to support researchers to get the tools and skills they need to work with this kind of data, we are also supporting some researchers who are creating those tools through our concept grant program.</p>
<p>We are trying to build some bridges amongst many stakeholders, so we have been working on some events to bring some key people together to discuss these issues and we have collaborated with representatives from the software industry like O’Reilly media and Facebook, as well as with funders like the SLOAN Foundation in the US and NESTA in the UK.</p>
<p>Finally, we think some of the tools that we are seeing emerge could have a role to play in the library space, and that’s something that we are going to be continuing to work on over the next year or so.</p>
<p>Episode 4: Looking Ahead</p>
<ol>
<li>*You’ve been making attempts to get students and faculty to free their data up—not hiding it. When it’s open and available, what are the benefits to that? How can using open repositories and open source improve the process for more people? *</li>
</ol>
<p>-&gt; comment about being able to re-use your own data later.</p>
<p>-&gt; also evidence in STEM that open data leads to more citations,</p>
<p>-&gt; opens up pathway to collaboration. Social sciences remain behind the curve on this</p>
<ol start="2">
<li><em>Researchers are very protective of their data and they often build data tools to facilitate their research, but the funding is for the primary research, not the tools. Subsequently, the tech is often left to expire. How are those two characteristics hindering progress with big data research and support within the academy? How was the Apache SPARK an exception to this pattern? What needs to happen moving forward to build sustainability into the process?</em></li>
</ol>
<p>COSMOS based out of Cardiff in the UK, built a tool for social media analysis. The funding ran out and now that tool is no longer being updated. Researcher who had been using the tool need to find alternatives, new researchers coming in to the field will use different tools, leading to heterogeneity in the analysis. This is an obvious example of the challenge that lack of funding for tools presents. It’s a long standing problem across all of research. There might be space to provide more support to researcher in helping them bridge form tool creation to the creation of small business that can support sustaining tools, but clearly not all tool creators will want to go down that route.</p>
<p>Another route is to try to get a large enough community involved to support the tooling.  <a href="https://biojs.net">BioJS</a> in the life science are doing well by building on top of many exiting research networks in the life sciences.</p>
<p>Spark gained rapid adoption outside of the research community as it solved a problem that many people who had been using Hadoop were facing - being able to quickly run computations across distributed data, without being blocked by disk reads. There are some R and python libraries that have had a similar path, and the general trend is that they tend to be highly useful within a commerical or startup context which leads to more contributors.</p>
<p>That’s not always the case, even for very popular open source tools (like homebrew), so this is far from a solved problem, and what makes it particularly hard in academia is that researchers rarely get funding for tools, in contrast to finding for projects.</p>
<p>Groups like the <a href="https://www.software.ac.uk">Software Sustainability Institute</a> in the UK are trying to help address this problem.</p>

 </div>

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/">This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>

 </article>



  </main>





</body>
</html>
